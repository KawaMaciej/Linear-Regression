{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e86ffa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.linear import LinearRegression\n",
    "from models.logistic import LogisticRegression\n",
    "from metrics.classification_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b2636c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "np.random.seed(42)\n",
    "\n",
    "classes = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "random_classes = np.random.choice(classes, size=n_samples)\n",
    "\n",
    "\n",
    "class_to_int = {label: idx for idx, label in enumerate(classes)}\n",
    "random_classes_int = np.vectorize(class_to_int.get)(random_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf74d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(2, 2, n_samples)\n",
    "Z = np.random.normal(9, 1, n_samples) \n",
    "P = np.random.beta(1, 2, n_samples)\n",
    "\n",
    "Y = 2*X +  np.random.normal(0, 0.5, n_samples) + P + Z + random_classes_int * X \n",
    "\n",
    "X= np.column_stack((X,X**2, Z + X, X * P, P, random_classes_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a214669",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression(regularization=\"None\").fit(X,Y)\n",
    "ridge = LinearRegression(regularization=\"Ridge\").fit(X,Y)\n",
    "lasso = LinearRegression(regularization=\"Lasso\", n_iter = 10000, lr=0.001).fit(X,Y)\n",
    "elastic = LinearRegression(regularization=\"ElasticNet\", n_iter = 10000, lr=0.001, alpha=0.4).fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8738d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═════════╕\n",
      "│ Metric   │   Value │\n",
      "╞══════════╪═════════╡\n",
      "│ MAE      │  1.9092 │\n",
      "├──────────┼─────────┤\n",
      "│ RMSE     │  2.753  │\n",
      "├──────────┼─────────┤\n",
      "│ MSE      │  7.5791 │\n",
      "╘══════════╧═════════╛\n",
      "╒══════════╤═════════╕\n",
      "│ Metric   │   Value │\n",
      "╞══════════╪═════════╡\n",
      "│ MAE      │  1.7339 │\n",
      "├──────────┼─────────┤\n",
      "│ RMSE     │  2.4169 │\n",
      "├──────────┼─────────┤\n",
      "│ MSE      │  5.8413 │\n",
      "╘══════════╧═════════╛\n",
      "╒══════════╤═════════╕\n",
      "│ Metric   │   Value │\n",
      "╞══════════╪═════════╡\n",
      "│ MAE      │  1.721  │\n",
      "├──────────┼─────────┤\n",
      "│ RMSE     │  2.3877 │\n",
      "├──────────┼─────────┤\n",
      "│ MSE      │  5.7009 │\n",
      "╘══════════╧═════════╛\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array(0.87876916, dtype=float32),\n",
       " Array(0.90656686, dtype=float32),\n",
       " np.float64(0.9088114287261543))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_pred = elastic.predict(X)\n",
    "elastic.print_errors(Y, elastic_pred), lasso.print_errors(Y, lasso.predict(X)), ridge.print_errors(Y, ridge.predict(X))\n",
    "elastic.score(X, Y), lasso.score(X, Y), ridge.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2561b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elastic.do_all(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3fdbda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg.Cooks_distance(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8439a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"A\", \"B\"]\n",
    "n_samples = 15\n",
    "\n",
    "random_classes = np.random.choice(classes, size=n_samples)\n",
    "\n",
    "\n",
    "class_to_int = {label: idx for idx, label in enumerate(classes)}\n",
    "random_classes_int = np.vectorize(class_to_int.get)(random_classes)\n",
    "\n",
    "X = np.random.normal(2, 2, n_samples)\n",
    "Z = np.random.normal(9, 1, n_samples) \n",
    "P = np.random.beta(1, 2, n_samples)\n",
    "\n",
    "\n",
    "X= np.column_stack((X, Z))\n",
    "\n",
    "Y = random_classes_int\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bfbbf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#X = pd.DataFrame(X, columns=[\"1\",\"2\",\"3\",\"4\",\"5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a0ce156",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (30) to match target batch_size (15).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m log = LogisticRegression(X.shape[\u001b[32m1\u001b[39m], \n\u001b[32m      2\u001b[39m                          \u001b[38;5;28mlen\u001b[39m(classes), \n\u001b[32m      3\u001b[39m                          \u001b[38;5;66;03m#regularization=\"l1\", \u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m                          lr=\u001b[32m0.0001\u001b[39m, \n\u001b[32m      7\u001b[39m                          n_iter = \u001b[32m100\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m log.fit(X, Y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kawam\\OneDrive\\Pulpit\\repo\\Linear Regression\\models\\logistic.py:143\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, Y)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28mself\u001b[39m.X = \u001b[38;5;28mself\u001b[39m._add_bias(X)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.solver == \u001b[33m\"\u001b[39m\u001b[33mGD\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28mself\u001b[39m.theta = GradientDescent(\u001b[38;5;28mself\u001b[39m.cross_entropy, \u001b[38;5;28mself\u001b[39m.theta, \u001b[38;5;28mself\u001b[39m.lr, \u001b[38;5;28mself\u001b[39m.n_iter)\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.solver == \u001b[33m\"\u001b[39m\u001b[33mLBFGS\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m.theta = LBFGS(\u001b[38;5;28mself\u001b[39m.cross_entropy, \u001b[38;5;28mself\u001b[39m.theta, \u001b[38;5;28mself\u001b[39m.lr, \u001b[38;5;28mself\u001b[39m.n_iter, \u001b[38;5;28mself\u001b[39m.m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kawam\\OneDrive\\Pulpit\\repo\\Linear Regression\\solvers\\grad_methods.py:7\u001b[39m, in \u001b[36mGradientDescent\u001b[39m\u001b[34m(func, init_x, learning_rate, n_iter)\u001b[39m\n\u001b[32m      5\u001b[39m x = torch.tensor(init_x, requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     y = func(x)\n\u001b[32m      8\u001b[39m     y.backward()\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kawam\\OneDrive\\Pulpit\\repo\\Linear Regression\\models\\logistic.py:119\u001b[39m, in \u001b[36mLogisticRegression.cross_entropy\u001b[39m\u001b[34m(self, theta_flat)\u001b[39m\n\u001b[32m    115\u001b[39m theta = theta_flat.view(\u001b[38;5;28mself\u001b[39m.n_features + \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.n_classes)\n\u001b[32m    117\u001b[39m logits = X @ theta  \n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m loss = F.cross_entropy(logits, Y)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.regularization == \u001b[33m\"\u001b[39m\u001b[33ml2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    122\u001b[39m     loss += \u001b[38;5;28mself\u001b[39m.alpha / \u001b[32m2\u001b[39m * torch.sum(theta[\u001b[32m1\u001b[39m:, :] ** \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kawam\\anaconda3\\envs\\app\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3478\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.cross_entropy_loss(\n\u001b[32m   3480\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3481\u001b[39m     target,\n\u001b[32m   3482\u001b[39m     weight,\n\u001b[32m   3483\u001b[39m     _Reduction.get_enum(reduction),\n\u001b[32m   3484\u001b[39m     ignore_index,\n\u001b[32m   3485\u001b[39m     label_smoothing,\n\u001b[32m   3486\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Expected input batch_size (30) to match target batch_size (15)."
     ]
    }
   ],
   "source": [
    "\n",
    "log = LogisticRegression(X.shape[1], \n",
    "                         len(classes), \n",
    "                         #regularization=\"l1\", \n",
    "                         solver=\"GD\",\n",
    "                         alpha=0,\n",
    "                         lr=0.0001, \n",
    "                         n_iter = 100)\n",
    "log.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a111b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.45535714)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = log.predict(X)\n",
    "pred\n",
    "balanced_accuracy(Y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53176c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49572236, -0.13727251],\n",
       "       [ 0.65168192,  1.51903648]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ec809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0\n",
      "0.6428571428571428 1\n",
      "0.7232142857142857 2\n",
      "0.5 3\n",
      "0.6517857142857143 4\n",
      "0.5178571428571428 5\n",
      "0.7232142857142857 6\n",
      "0.5178571428571428 7\n",
      "0.7232142857142857 8\n",
      "0.5178571428571428 9\n",
      "0.7857142857142857 10\n",
      "0.5178571428571428 11\n",
      "0.7857142857142857 12\n",
      "0.5178571428571428 13\n",
      "0.8571428571428572 14\n",
      "0.5178571428571428 15\n",
      "0.7946428571428572 16\n",
      "0.5178571428571428 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kawam\\anaconda3\\envs\\app\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\kawam\\anaconda3\\envs\\app\\Lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from models.onevsall import OVA\n",
    "from models.svm import SVMClassificator\n",
    "for i in range(0, 18):\n",
    "    ova = OVA(SVMClassificator(kernel=\"poly\", n_iter=i, lr=0.01 , C=i, r=1, degree=2 )).fit(X,Y)\n",
    "\n",
    "    print(balanced_accuracy(Y, ova.predict(X)), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb75e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7321428571428572\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m svm.predict(X)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(balanced_accuracy(Y, svm.predict(X)))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m svm.plot()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kawam\\OneDrive\\Pulpit\\repo\\Linear Regression\\models\\svm.py:80\u001b[39m, in \u001b[36mSVMClassificator.plot\u001b[39m\u001b[34m(self, title)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot\u001b[39m(\u001b[38;5;28mself\u001b[39m, title=\u001b[33m'\u001b[39m\u001b[33mPlot for non linear SVM\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     plt.scatter(\u001b[38;5;28mself\u001b[39m.X[:, \u001b[32m0\u001b[39m], \u001b[38;5;28mself\u001b[39m.X[:, \u001b[32m1\u001b[39m], c=\u001b[38;5;28mself\u001b[39m.y, s=\u001b[32m50\u001b[39m, cmap=\u001b[33m'\u001b[39m\u001b[33mwinter\u001b[39m\u001b[33m'\u001b[39m, alpha=\u001b[32m.5\u001b[39m)\n\u001b[32m     81\u001b[39m     ax = plt.gca()\n\u001b[32m     82\u001b[39m     xlim = ax.get_xlim()\n",
      "\u001b[31mIndexError\u001b[39m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "svm = SVMClassificator(kernel=\"sigmoid\", n_iter=100, lr=0.01 , C=100, r=1, degree=2 ).fit(X,Y)\n",
    "svm.predict(X)\n",
    "print(balanced_accuracy(Y, svm.predict(X)))\n",
    "svm.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
